{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208cc987-3d19-402e-b4bb-d80bacf3c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "# from fbpn_sr_rbpn_v1 import Net as FBPNSR_RBPN_V1\n",
    "# from fbpn_sr_rbpn_v2 import Net as FBPNSR_RBPN_V2\n",
    "# from fbpn_sr_rbpn_v3 import Net as FBPNSR_RBPN_V3\n",
    "# from fbpn_sr_rbpn_v4 import Net as FBPNSR_RBPN_V4\n",
    "# from fbpn_sr_rbpn_v1_ref import Net as FBPNSR_RBPN_V1_REF\n",
    "# from fbpn_sr_rbpn_v2_ref import Net as FBPNSR_RBPN_V2_REF\n",
    "# from fbpn_sr_rbpn_v3_ref import Net as FBPNSR_RBPN_V3_REF\n",
    "from fbpn_sr_rbpn_v4_ref import Net as FBPNSR_RBPN_V4_REF, FeatureExtractor\n",
    "# from data import get_test_set_interp\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import utils\n",
    "import time\n",
    "import cv2\n",
    "import math\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627f8413-55c0-4c02-bbd0-d224b06511bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(chop_forward=False, data_dir='vimeo_triplet/sequences', f='/home/mohomin123/.local/share/jupyter/runtime/kernel-7b611cde-02d6-4721-8614-9823736a6585.json', file_list='tri_testlist.txt', gpu_mode=False, gpus=0, model='weights/FBPNSR_RBPN_V4_REF_Lf_STAR_T_HR.pth', model_type='FBPNSR_RBPN_V4_REF', output='Results_T_SR_HR/', residual=False, seed=123, testBatchSize=1, threads=1, upscale_factor=4)\n",
      "===> Building model  FBPNSR_RBPN_V4_REF\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Super Res Example')\n",
    "parser.add_argument('--upscale_factor', type=int, default=4, help=\"super resolution upscale factor\")\n",
    "parser.add_argument('--testBatchSize', type=int, default=1, help='testing batch size')\n",
    "parser.add_argument('--gpu_mode', type=bool, default=False) ##\n",
    "parser.add_argument('--chop_forward', type=bool, default=False)\n",
    "parser.add_argument('--threads', type=int, default=1, help='number of threads for data loader to use')\n",
    "parser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\n",
    "parser.add_argument('--gpus', default=0, type=float, help='number of gpu') ##\n",
    "parser.add_argument('--data_dir', type=str, default='vimeo_triplet/sequences')\n",
    "parser.add_argument('--file_list', type=str, default='tri_testlist.txt')\n",
    "parser.add_argument('--model_type', type=str, default='FBPNSR_RBPN_V4_REF')\n",
    "parser.add_argument('--residual', type=bool, default=False)\n",
    "parser.add_argument('--output', default='Results_T_SR_HR/', help='Location to save checkpoint models')\n",
    "parser.add_argument('--model', default='weights/FBPNSR_RBPN_V4_REF_Lf_STAR_T_HR.pth', help='sr pretrained base model')\n",
    "parser.add_argument('-f')\n",
    "\n",
    "opt = parser.parse_args()\n",
    "\n",
    "gpus_list=range(opt.gpus)\n",
    "print(opt)\n",
    "\n",
    "cuda = opt.gpu_mode\n",
    "if cuda and not torch.cuda.is_available():\n",
    "    raise Exception(\"No GPU found, please run without --cuda\")\n",
    "\n",
    "torch.manual_seed(opt.seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(opt.seed)\n",
    "\n",
    "print('===> Building model ', opt.model_type)\n",
    "if opt.model_type == 'FBPNSR_RBPN_V4_REF':\n",
    "    # model_dict = torch.load(opt.model)\n",
    "    # model = torch.nn.Module.load_state_dict(model_dict)\n",
    "    \n",
    "    model = FBPNSR_RBPN_V4_REF(base_filter=256,  feat = 64, num_stages=3, n_resblock=5, scale_factor=opt.upscale_factor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd5bc9-781c-458a-9001-23ee3d0465da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model = torch.nn.DataParallel(model, device_ids=gpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f7c69-fcb0-4fbb-8e34-38868eee1db3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_network(net):\n",
    "    num_params = 0\n",
    "    for param in net.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(net)\n",
    "    print('Total number of parameters: %d' % num_params)\n",
    "\n",
    "print('---------- Networks architecture -------------')\n",
    "print_network(model)\n",
    "print('----------------------------------------------')\n",
    "\n",
    "model.load_state_dict(torch.load(opt.model, map_location=lambda storage, loc: storage))\n",
    "print('Pre-trained SR model is loaded.')\n",
    "\n",
    "if cuda:\n",
    "    model = model.cuda(gpus_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5bd3cd-869e-47e6-a5da-01ad6e2cfa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval():\n",
    "    model.eval()\n",
    "    avg_psnr_predicted = 0.0\n",
    "    for batch in testing_data_loader:\n",
    "        input, flow_f, flow_b, filename, d_dir = batch[0], batch[1], batch[2], batch[3], batch[4]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            t_im1 = Variable(input[0]).cuda(gpus_list[0])\n",
    "            t_im2 = Variable(input[1]).cuda(gpus_list[0])\n",
    "            t_flow_f = Variable(flow_f).cuda(gpus_list[0]).float()\n",
    "            t_flow_b = Variable(flow_b).cuda(gpus_list[0]).float()\n",
    "            \n",
    "        t0 = time.time()                \n",
    "        if opt.chop_forward:\n",
    "            with torch.no_grad():\n",
    "                pred_l  = chop_forward(t_im1, t_im2, t_flow_f, t_flow_b, model)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                _, _, _, pred_l  = model(t_im1, t_im2, t_flow_f, t_flow_b, train=False)\n",
    "            \n",
    "        t1 = time.time()\n",
    "        \n",
    "        print(\"===> Processing: %s || Timer: %.4f sec.\" % (d_dir[0]+'/frame10i11.png', (t1 - t0)))\n",
    "        pred_l = utils.denorm(pred_l[0].cpu().data, vgg=True)\n",
    "        pred_1 = utils.denorm(t_im1[0].cpu().data, vgg=True)\n",
    "        pred_2 = utils.denorm(t_im2[0].cpu().data, vgg=True)\n",
    "\n",
    "        if opt.data_dir == 'ucf101_interp_ours':\n",
    "            save_img(pred_1, d_dir[0],'frame_00.png', False)\n",
    "            save_img(pred_l, d_dir[0],'frame_01_gt.png', False)\n",
    "            save_img(pred_2, d_dir[0],'frame_02.png', False)\n",
    "        else:\n",
    "            save_img(pred_1, d_dir[0],'im1.png', False)\n",
    "            save_img(pred_l, d_dir[0],'im2.png', False)\n",
    "            save_img(pred_2, d_dir[0],'im3.png', False)\n",
    "        \n",
    "        \n",
    "def save_img(img, d_dir,img_name, pred_flag):\n",
    "    save_img = img.squeeze().clamp(0, 1).numpy().transpose(1,2,0)\n",
    "    filename = os.path.splitext(img_name)\n",
    "\n",
    "    # save img\n",
    "    save_dir=os.path.join(opt.output, d_dir)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    if pred_flag:\n",
    "        save_fn = save_dir +'/'+ filename[0]+'_'+opt.model_type+filename[1]\n",
    "    else:\n",
    "        save_fn = save_dir +'/'+ img_name\n",
    "    cv2.imwrite(save_fn, cv2.cvtColor(save_img*255, cv2.COLOR_BGR2RGB),  [cv2.IMWRITE_PNG_COMPRESSION, 0])\n",
    "    \n",
    "def chop_forward(t_im1, t_im2, t_flow_f, t_flow_b, model, shave=8, min_size=200000, nGPUs=opt.gpus):\n",
    "    b, c, h, w = t_im1.size()\n",
    "    h_half, w_half = h // 2, w // 2\n",
    "    h_size, w_size = h_half + shave, w_half + shave\n",
    "    \n",
    "    if h_size%2:\n",
    "        h_size = h_size + 1\n",
    "\n",
    "    if w_size%2:\n",
    "        w_size = w_size + 1\n",
    "        \n",
    "    inputlist = [\n",
    "        [t_im1[:, :, 0:h_size, 0:w_size], t_im2[:, :, 0:h_size, 0:w_size], t_flow_f[:, :, 0:h_size, 0:w_size], t_flow_b[:, :, 0:h_size, 0:w_size]],\n",
    "        [t_im1[:, :, 0:h_size, (w - w_size):w],t_im2[:, :, 0:h_size, (w - w_size):w],t_flow_f[:, :, 0:h_size, (w - w_size):w],t_flow_b[:, :, 0:h_size, (w - w_size):w] ],\n",
    "        [t_im1[:, :, (h - h_size):h, 0:w_size],t_im2[:, :, (h - h_size):h, 0:w_size],t_flow_f[:, :, (h - h_size):h, 0:w_size],t_flow_b[:, :, (h - h_size):h, 0:w_size] ],\n",
    "        [t_im1[:, :, (h - h_size):h,  (w - w_size):w],t_im2[:, :, (h - h_size):h,  (w - w_size):w],t_flow_f[:, :, (h - h_size):h,  (w - w_size):w],t_flow_b[:, :, (h - h_size):h,  (w - w_size):w] ]]\n",
    "\n",
    "    if w_size * h_size < min_size:\n",
    "        outputlist = []\n",
    "        for i in range(0, 4, nGPUs):\n",
    "            with torch.no_grad():\n",
    "                input_batch = inputlist[i]\n",
    "                _, _, _, output_batch = model(input_batch[0], input_batch[1], input_batch[2], input_batch[3], train=False)\n",
    "            outputlist.extend(output_batch.chunk(nGPUs, dim=0))\n",
    "    else:\n",
    "        outputlist = [\n",
    "            chop_forward(patch[0], patch[1], patch[2],patch[3], model, shave, min_size, nGPUs) \\\n",
    "            for patch in inputlist]\n",
    "\n",
    "    scale=1\n",
    "    h, w = scale * h, scale * w\n",
    "    h_half, w_half = scale * h_half, scale * w_half\n",
    "    h_size, w_size = scale * h_size, scale * w_size\n",
    "    shave *= scale\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = Variable(t_im1.data.new(b, c, h, w))\n",
    "    output[:, :, 0:h_half, 0:w_half] \\\n",
    "        = outputlist[0][:, :, 0:h_half, 0:w_half]\n",
    "    output[:, :, 0:h_half, w_half:w] \\\n",
    "        = outputlist[1][:, :, 0:h_half, (w_size - w + w_half):w_size]\n",
    "    output[:, :, h_half:h, 0:w_half] \\\n",
    "        = outputlist[2][:, :, (h_size - h + h_half):h_size, 0:w_half]\n",
    "    output[:, :, h_half:h, w_half:w] \\\n",
    "        = outputlist[3][:, :, (h_size - h + h_half):h_size, (w_size - w + w_half):w_size]\n",
    "\n",
    "    return output\n",
    "\n",
    "##Eval Start!!!!\n",
    "# eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8b5c4-1e45-48a7-b4ee-c1fc504e0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_test_set_interp\n",
    "print('===> Loading datasets')\n",
    "test_set = get_test_set_interp(opt.data_dir, opt.file_list)\n",
    "testing_data_loader = DataLoader(dataset=test_set, num_workers=opt.threads, batch_size=opt.testBatchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70385843-e6f3-4882-8980-71e2244f8088",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f930d7-3a9c-491b-b3ac-c6dec6eb8fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d641a-7df2-43b6-bb1d-9619b5fb219f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733b165-3aeb-463f-9c6b-5f2aaf7e07a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video_capture",
   "language": "python",
   "name": "video_capture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
